{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import models, transforms\n",
    "from torchvision.ops import nms\n",
    "from deep_sort_realtime.deep_sort.track import Track\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_classes_90 = [\"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\",\n",
    "            \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "            \"unknown\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\",\n",
    "            \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"unknown\", \"backpack\",\n",
    "            \"umbrella\", \"unknown\", \"unknown\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\",\n",
    "            \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
    "            \"surfboard\", \"tennis racket\", \"bottle\", \"unknown\", \"wine glass\", \"cup\", \"fork\", \"knife\",\n",
    "            \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\",\n",
    "            \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"unknown\", \"dining table\",\n",
    "            \"unknown\", \"unknown\", \"toilet\", \"unknown\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
    "            \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"unknown\",\n",
    "            \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\" ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing device to load the model and frame in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#CUDA on Nvidia\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(device)\n",
    "#Apple\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps)')\n",
    "    print(device)\n",
    "#DirectML (Windows only, on DX12 supported cards)\n",
    "elif importlib.util.find_spec(\"torch_directml\") is not None:\n",
    "    import torch_directml\n",
    "    device = torch_directml.device()\n",
    "    print(torch_directml.device_name(0))\n",
    "#Fallback to CPU\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video writer wrapped around for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_writer(video_cap, output_filename):\n",
    "    # grab the width, height, and fps of the frames in the video stream.\n",
    "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # initialize the FourCC and a video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    writer = cv2.VideoWriter(output_filename, fourcc, fps,\n",
    "                             (frame_width, frame_height))\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the Object Detection Model and DeepSORT model\n",
    "Two choices for models:\n",
    "1. COCO trained FasterRCNN with ResNet50 backbone\n",
    "2. COCO trained FasterRCNN with ResNet50 backbone that was fine-tuned with Fudan Pedestrian dataset, and classifies only pedestrians(COCO person class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#Default PyTorch weights\n",
    "model = models.detection.fasterrcnn_resnet50_fpn_v2(weights=models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT).to(device)\n",
    "#Model with default weights fine-tuned to detect only pedestrians\n",
    "#model = torch.load('model_weights/model_state_dict.pt', weights_only=False).to(device)\n",
    "model = model.eval()\n",
    "deepsort = DeepSort(max_age=60, max_iou_distance=0.5, n_init=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load video paths\n",
    "Both input and output paths are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video is in /storage/Programming/projects/ObjectTracking/video/macv-obj-tracking-video.mp4\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "video_path = Path.joinpath(Path.cwd(), 'video/macv-obj-tracking-video.mp4')\n",
    "output_path = Path.joinpath(Path.cwd(), 'video/output.mp4')\n",
    "print(f\"Video is in {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Do the inference and perform DeepSORT\n",
    "Non max supression is applied by this implementation of DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(str(video_path))\n",
    "outfile = create_video_writer(cap, str(output_path))\n",
    "frame_count = 0\n",
    "object_times = {}\n",
    "score_threshold = 0.9\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Preprocess the frame\n",
    "    pil_img = Image.fromarray(frame)\n",
    "    img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Do the inference with the model\n",
    "    with torch.no_grad():\n",
    "        detections = model(img_tensor)\n",
    "    boxes = detections[0]['boxes'].cpu().numpy()\n",
    "    labels = detections[0]['labels'].cpu().numpy()\n",
    "    scores = detections[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter the detections by a score threshold\n",
    "    valid_boxes = boxes[scores > score_threshold]\n",
    "    valid_scores = scores[scores > score_threshold]\n",
    "    valid_cls_ids = labels[scores > score_threshold]\n",
    "\n",
    "    valid_boxes_tensor = torch.tensor(valid_boxes, dtype=torch.float32)\n",
    "    valid_scores_tensor = torch.tensor(valid_scores, dtype=torch.float32)\n",
    "    valid_cls_ids_tensor = torch.tensor(valid_cls_ids, dtype=torch.int64)\n",
    "\n",
    "    # Prepare detections for DeepSORT (box format: x1, y1, width, height, score)\n",
    "    detections_deepsort = []\n",
    "    for box, score, label in zip(valid_boxes_tensor, valid_scores_tensor, valid_cls_ids_tensor):\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        detections_deepsort.append([[x1, y1, width, height], score, label])\n",
    "    \n",
    "    # Update DeepSORT tracker with the current frame's detections\n",
    "    trackers: list[Track] = deepsort.update_tracks(detections_deepsort, frame=frame)\n",
    "\n",
    "    # Draw bounding boxes, trajectory lines, and bounding box information\n",
    "    for track in trackers:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        det_cls = track.det_class\n",
    "        track_id = track.track_id\n",
    "        if track_id not in object_times:\n",
    "            object_times[track_id] = {\"cls\": det_cls,\n",
    "                                      \"cls_name\": coco_classes_90[det_cls], \n",
    "                                      \"trajectory\": [], \n",
    "                                      \"entry_time\": cap.get(cv2.CAP_PROP_POS_MSEC), \n",
    "                                      \"exit_time\": None}\n",
    "            \n",
    "        object_times[track_id][\"exit_time\"] = cap.get(cv2.CAP_PROP_POS_MSEC)  # Update exit time every frame the object is tracked\n",
    "\n",
    "        x1, y1, x2, y2 = track.to_tlbr()  # Returns bounding box in the (x1, y1, x2, y2) format\n",
    "\n",
    "        #Draw trajectory of the bounding box\n",
    "        object_times[track_id][\"trajectory\"].append((x1,y1,x2,y2))\n",
    "        traj = object_times[track_id][\"trajectory\"]\n",
    "        if(len(traj) >= 2):\n",
    "            for i in range(1, len(traj)):\n",
    "                cv2.line(frame, (int(traj[i-1][0]), int(traj[i-1][1])), \n",
    "                     (int(traj[i][0]), int(traj[i][1])), (0, 0, 255), 2)\n",
    "                \n",
    "        #Draw bounding box\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id} Cls = {det_cls}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Tracking\", frame)\n",
    "    #Write the output video\n",
    "    outfile.write(frame)\n",
    "\n",
    "    # Break loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "outfile.release()\n",
    "cv2.destroyAllWindows()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1 person appeared from time 100.000 ms to time 1833.333 ms for a of 1733.333 ms.\n",
      "ID: 2 person appeared from time 100.000 ms to time 1833.333 ms for a of 1733.333 ms.\n",
      "ID: 3 person appeared from time 100.000 ms to time 1833.333 ms for a of 1733.333 ms.\n",
      "ID: 4 person appeared from time 100.000 ms to time 1833.333 ms for a of 1733.333 ms.\n",
      "ID: 5 handbag appeared from time 100.000 ms to time 1833.333 ms for a of 1733.333 ms.\n",
      "ID: 8 person appeared from time 133.333 ms to time 1833.333 ms for a of 1700.000 ms.\n",
      "ID: 9 person appeared from time 200.000 ms to time 1833.333 ms for a of 1633.333 ms.\n",
      "ID: 11 person appeared from time 466.667 ms to time 1833.333 ms for a of 1366.667 ms.\n",
      "ID: 12 person appeared from time 533.333 ms to time 1833.333 ms for a of 1300.000 ms.\n",
      "ID: 22 person appeared from time 1466.667 ms to time 1833.333 ms for a of 366.667 ms.\n"
     ]
    }
   ],
   "source": [
    "# Print the duration for each object (frame range)\n",
    "for obj_id, times in object_times.items():\n",
    "    entry_time = times[\"entry_time\"]\n",
    "    exit_time = times[\"exit_time\"]\n",
    "    duration = exit_time - entry_time\n",
    "    print(f\"ID: {obj_id} {times['cls_name']} appeared from time {entry_time:.3f} ms to time {exit_time:.3f} ms for a of {duration:.3f} ms.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
