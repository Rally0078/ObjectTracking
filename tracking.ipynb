{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import models, transforms\n",
    "from torchvision.ops import nms\n",
    "from deep_sort_realtime.deep_sort.track import Track\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radeon RX Vega\u0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(device)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps)')\n",
    "    print(device)\n",
    "elif importlib.util.find_spec(\"torch_directml\") is not None:\n",
    "    import torch_directml\n",
    "    device = torch_directml.device()\n",
    "    print(torch_directml.device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_writer(video_cap, output_filename):\n",
    "\n",
    "    # grab the width, height, and fps of the frames in the video stream.\n",
    "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # initialize the FourCC and a video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    writer = cv2.VideoWriter(output_filename, fourcc, fps,\n",
    "                             (frame_width, frame_height))\n",
    "\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn_v2(weights=models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT).to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_wts_path))\n"
     ]
    }
   ],
   "source": [
    "deepsort = DeepSort(max_age=30)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "video_path = Path(\"./video/macv-obj-tracking-video.mp4\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m     detections_deepsort\u001b[38;5;241m.\u001b[39mappend([[x1, y1, width, height], score, label])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Update DeepSORT tracker with the current frame's detections\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m trackers: \u001b[38;5;28mlist\u001b[39m[Track] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepsort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections_deepsort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and tracking IDs\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m trackers:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:199\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[1;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[0m\n\u001b[0;32m    196\u001b[0m raw_detections \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_detections \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_detections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Proper deep sort detection objects that consist of bbox, confidence and embedding.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_detections(raw_detections, embeds, instance_masks\u001b[38;5;241m=\u001b[39minstance_masks, others\u001b[38;5;241m=\u001b[39mothers)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:246\u001b[0m, in \u001b[0;36mDeepSort.generate_embeds\u001b[1;34m(self, frame, raw_dets, instance_masks)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder\u001b[38;5;241m.\u001b[39mpredict(masked_crops)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrops\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:127\u001b[0m, in \u001b[0;36mMobileNetv2_Embedder.predict\u001b[1;34m(self, np_images)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mbatch inference\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m all_feats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 127\u001b[0m preproc_imgs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m np_images]\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m this_batch \u001b[38;5;129;01min\u001b[39;00m batch(preproc_imgs, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size):\n\u001b[0;32m    130\u001b[0m     this_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(this_batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:97\u001b[0m, in \u001b[0;36mMobileNetv2_Embedder.preprocess\u001b[1;34m(self, np_image)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     np_image_rgb \u001b[38;5;241m=\u001b[39m np_image\n\u001b[1;32m---> 97\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_image_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINPUT_WIDTH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m trans \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m     99\u001b[0m     [\n\u001b[0;32m    100\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m     ]\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m input_image \u001b[38;5;241m=\u001b[39m trans(input_image)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = 0\n",
    "object_times = {}\n",
    "score_threshold = 0.85\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    # Preprocess the frame and detect objects using Faster R-CNN\n",
    "    pil_img = Image.fromarray(frame)\n",
    "    img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        detections = model(img_tensor)\n",
    "    # Get bounding boxes, classes, and scores (only keep detections with score > 0.5)\n",
    "    boxes = detections[0]['boxes'].cpu().numpy()\n",
    "    labels = detections[0]['labels'].cpu().numpy()\n",
    "    scores = detections[0]['scores'].cpu().numpy()\n",
    "\n",
    "    valid_boxes = boxes[scores > score_threshold]\n",
    "    valid_scores = scores[scores > score_threshold]\n",
    "    valid_cls_ids = labels[scores > score_threshold]\n",
    "    valid_boxes_tensor = torch.tensor(valid_boxes, dtype=torch.float32)\n",
    "    valid_scores_tensor = torch.tensor(valid_scores, dtype=torch.float32)\n",
    "    valid_cls_ids_tensor = torch.tensor(valid_cls_ids, dtype=torch.int64)\n",
    "\n",
    "    nms_indices = nms(valid_boxes_tensor, valid_scores_tensor, iou_threshold=0.4)\n",
    "    final_boxes = valid_boxes_tensor[nms_indices].cpu().numpy()\n",
    "    final_scores = valid_scores_tensor[nms_indices].cpu().numpy()\n",
    "    final_cls_ids = valid_cls_ids_tensor[nms_indices].cpu().numpy()\n",
    "\n",
    "    # Prepare detections for DeepSORT (format: x1, y1, width, height, score)\n",
    "    detections_deepsort = []\n",
    "    for box, score, label in zip(final_boxes, final_scores, final_cls_ids):\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        detections_deepsort.append([[x1, y1, width, height], score, label])\n",
    "    # Update DeepSORT tracker with the current frame's detections\n",
    "    trackers: list[Track] = deepsort.update_tracks(detections_deepsort, frame=frame)\n",
    "\n",
    "    # Draw bounding boxes and tracking IDs\n",
    "    for track in trackers:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        det_cls = track.det_class\n",
    "        track_id = track.track_id\n",
    "        if track_id not in object_times:\n",
    "            object_times[track_id] = {\"entry_frame\": frame_count, \"exit_frame\": None}\n",
    "        object_times[track_id][\"exit_frame\"] = frame_count  # Update exit frame every frame the object is tracked\n",
    "        x1, y1, w, h = track.to_tlbr()  # DeepSORT returns bounding box as (x1, y1, x2, y2)\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id} Cls = {det_cls}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Object Tracking\", frame)\n",
    "\n",
    "    # Break loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object ID: 1 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 2 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 3 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 4 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 5 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 6 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 7 appeared from frame 3 to frame 37 for 34 frames.\n",
      "Object ID: 9 appeared from frame 3 to frame 49 for 46 frames.\n",
      "Object ID: 10 appeared from frame 3 to frame 90 for 87 frames.\n",
      "Object ID: 12 appeared from frame 4 to frame 61 for 57 frames.\n",
      "Object ID: 15 appeared from frame 15 to frame 47 for 32 frames.\n",
      "Object ID: 16 appeared from frame 15 to frame 57 for 42 frames.\n",
      "Object ID: 20 appeared from frame 27 to frame 90 for 63 frames.\n",
      "Object ID: 25 appeared from frame 36 to frame 72 for 36 frames.\n",
      "Object ID: 26 appeared from frame 36 to frame 66 for 30 frames.\n",
      "Object ID: 40 appeared from frame 51 to frame 90 for 39 frames.\n",
      "Object ID: 41 appeared from frame 52 to frame 90 for 38 frames.\n",
      "Object ID: 42 appeared from frame 52 to frame 89 for 37 frames.\n",
      "Object ID: 46 appeared from frame 57 to frame 90 for 33 frames.\n",
      "Object ID: 49 appeared from frame 58 to frame 90 for 32 frames.\n",
      "Object ID: 50 appeared from frame 66 to frame 90 for 24 frames.\n",
      "Object ID: 51 appeared from frame 67 to frame 90 for 23 frames.\n",
      "Object ID: 52 appeared from frame 67 to frame 90 for 23 frames.\n",
      "Object ID: 53 appeared from frame 72 to frame 90 for 18 frames.\n",
      "Object ID: 57 appeared from frame 75 to frame 90 for 15 frames.\n",
      "Object ID: 60 appeared from frame 80 to frame 90 for 10 frames.\n"
     ]
    }
   ],
   "source": [
    "# Print the duration for each object (frame range)\n",
    "for obj_id, times in object_times.items():\n",
    "    entry_frame = times[\"entry_frame\"]\n",
    "    exit_frame = times[\"exit_frame\"]\n",
    "    duration = exit_frame - entry_frame\n",
    "    print(f\"Object ID: {obj_id} appeared from frame {entry_frame} to frame {exit_frame} for {duration} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3,640,640, device=device)\n",
    "print(X.shape)\n",
    "y = model(X.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
